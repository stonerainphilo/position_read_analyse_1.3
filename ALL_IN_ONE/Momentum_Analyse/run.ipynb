{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edc2ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --input INPUT --llp-params LLP_PARAMS\n",
      "                             [--output OUTPUT] [--grid-size GRID_SIZE]\n",
      "                             [--momentum-clusters MOMENTUM_CLUSTERS]\n",
      "                             [--samples-per-block SAMPLES_PER_BLOCK]\n",
      "                             [--min-mass MIN_MASS] [--max-mass MAX_MASS]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --input, --llp-params\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "批量处理脚本：process_all_data.py\n",
    "用法：python process_all_data.py --input /path/to/data --output ./results\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from blockify_sample import HierarchicalParticleBlocking, BlockConfig\n",
    "from block_decay import LLPAnalysisPipeline\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process particle data for LLP analysis')\n",
    "    parser.add_argument('--input', type=str, required=True, help='Input particle data file')\n",
    "    parser.add_argument('--llp-params', type=str, required=True, help='LLP parameters file')\n",
    "    parser.add_argument('--output', type=str, default='./results', help='Output directory')\n",
    "    parser.add_argument('--grid-size', type=int, default=10, help='Position grid size')\n",
    "    parser.add_argument('--momentum-clusters', type=int, default=8, help='Number of momentum clusters')\n",
    "    parser.add_argument('--samples-per-block', type=int, default=100, help='Samples per block for LLP analysis')\n",
    "    parser.add_argument('--min-mass', type=float, default=0.1, help='Minimum LLP mass to analyze')\n",
    "    parser.add_argument('--max-mass', type=float, default=5.0, help='Maximum LLP mass to analyze')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = Path(args.output)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Input file: {args.input}\")\n",
    "    print(f\"LLP params: {args.llp_params}\")\n",
    "    print(f\"Output directory: {args.output}\")\n",
    "    \n",
    "    # 配置\n",
    "    config = BlockConfig(\n",
    "        momentum_n_clusters=args.momentum_clusters,\n",
    "        position_bin_size=args.grid_size,\n",
    "        use_position_clustering=False,\n",
    "        compression='gzip'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # 步骤1: 数据分块\n",
    "        print(\"\\nStep 1: Creating particle blocks...\")\n",
    "        blocker = HierarchicalParticleBlocking(\n",
    "            data_path=args.input,\n",
    "            output_dir=str(output_dir / 'particle_blocks'),\n",
    "            config=config\n",
    "        )\n",
    "        blocks = blocker.create_blocks()\n",
    "        blocker.save_blocks(format='parquet')\n",
    "        \n",
    "        # 步骤2: LLP分析\n",
    "        print(\"\\nStep 2: Running LLP analysis...\")\n",
    "        pipeline = LLPAnalysisPipeline(\n",
    "            particle_data_file=args.input,\n",
    "            llp_params_file=args.llp_params,\n",
    "            blocks_output_dir=str(output_dir / 'particle_blocks'),\n",
    "            llp_output_dir=str(output_dir / 'llp_results')\n",
    "        )\n",
    "        \n",
    "        results = pipeline.run_full_analysis(\n",
    "            blocking_config=config,\n",
    "            samples_per_block=args.samples_per_block,\n",
    "            llp_mass_range=(args.min_mass, args.max_mass)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Results saved in: {output_dir}\")\n",
    "        \n",
    "        # 生成报告\n",
    "        report = f\"\"\"\n",
    "        Processing Report\n",
    "        =================\n",
    "        Input file: {args.input}\n",
    "        LLP parameters: {args.llp_params}\n",
    "        Total blocks created: {len(blocks)}\n",
    "        Total LLP parameters analyzed: {len(results) if results is not None else 0}\n",
    "        Output directory: {output_dir}\n",
    "        \n",
    "        Generated files:\n",
    "        - {output_dir}/particle_blocks/global_index.parquet\n",
    "        - {output_dir}/particle_blocks/blocks/*/particles.parquet\n",
    "        - {output_dir}/llp_results/llp_analysis_results.csv\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(output_dir / 'processing_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
