{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544499a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def merge_seed_csvs(folder_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    合并指定文件夹中不同seed的B_511和B_521 csv文件\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录，如果为None则输出到原文件夹\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 用于存储不同前缀的文件数据\n",
    "    file_groups = {}\n",
    "    \n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 只处理csv文件\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "            \n",
    "        # 匹配文件名模式：B_511_seedXXXX.csv 或 B_521_seedXXXX.csv\n",
    "        match = re.match(r'(B_5[12]1)_seed\\d+\\.csv', filename)\n",
    "        \n",
    "        if match:\n",
    "            prefix = match.group(1)  # 获取前缀 (B_511 或 B_521)\n",
    "            \n",
    "            try:\n",
    "                # 读取csv文件\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # 添加一个列来记录原始文件名（可选）\n",
    "                df['source_file'] = filename\n",
    "                \n",
    "                # 将数据添加到对应的分组中\n",
    "                if prefix not in file_groups:\n",
    "                    file_groups[prefix] = []\n",
    "                file_groups[prefix].append(df)\n",
    "                \n",
    "                print(f\"已读取: {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "    \n",
    "    # 合并并保存每个分组的数据\n",
    "    for prefix, dfs in file_groups.items():\n",
    "        if dfs:\n",
    "            try:\n",
    "                # 合并所有DataFrame\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "                # 输出文件路径\n",
    "                output_path = os.path.join(output_dir, f\"{prefix}.csv\")\n",
    "                \n",
    "                # 保存到csv文件\n",
    "                combined_df.to_csv(output_path, index=False)\n",
    "                print(f\"\\n已成功合并并保存: {output_path}\")\n",
    "                print(f\"合并了 {len(dfs)} 个文件\")\n",
    "                print(f\"总行数: {len(combined_df)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"合并 {prefix} 文件时出错: {e}\")\n",
    "        else:\n",
    "            print(f\"没有找到 {prefix} 相关的文件\")\n",
    "    \n",
    "    return file_groups\n",
    "\n",
    "def merge_seed_csvs_with_options(folder_path, output_dir=None, add_source_column=True):\n",
    "    \"\"\"\n",
    "    更灵活的合并函数，带有更多选项\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录\n",
    "    add_source_column (bool): 是否添加源文件名列\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 使用字典存储不同模式的文件\n",
    "    patterns = {\n",
    "        'B_511': re.compile(r'B_511_seed\\d+\\.csv'),\n",
    "        'B_521': re.compile(r'B_521_seed\\d+\\.csv')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matched_dfs = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            print(filename)\n",
    "            if pattern.match(filename):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(file_path)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    if add_source_column:\n",
    "                        df['source_file'] = filename\n",
    "                    \n",
    "                    matched_dfs.append(df)\n",
    "                    print(f\"已读取: {filename} -> {pattern_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "        \n",
    "        if matched_dfs:\n",
    "            # 合并数据\n",
    "            combined_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "            \n",
    "            # 保存文件\n",
    "            output_path = os.path.join(output_dir, f\"{pattern_name}.csv\")\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            results[pattern_name] = {\n",
    "                'file_count': len(matched_dfs),\n",
    "                'total_rows': len(combined_df),\n",
    "                'output_path': output_path\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✅ {pattern_name}: 合并了 {len(matched_dfs)} 个文件\")\n",
    "            print(f\"   总行数: {len(combined_df)}\")\n",
    "            print(f\"   保存到: {output_path}\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  未找到 {pattern_name} 相关的文件\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# # 使用示例\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 示例用法1：基本用法\n",
    "#     folder = \"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/14TeV_B/\"\n",
    "#     # result = merge_seed_csvs(folder)\n",
    "    \n",
    "#     # 示例用法2：指定输出目录\n",
    "#     result = merge_seed_csvs_with_options(folder, output_dir=\"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP\", add_source_column=False)\n",
    "    \n",
    "#     # 示例用法3：使用更灵活的函数（不添加源文件名列）\n",
    "#     # result = merge_seed_csvs_with_options(\n",
    "    #     folder, \n",
    "    #     output_dir=\"/path/to/output\",\n",
    "    #     add_source_column=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccdb61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 files into /media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP\n"
     ]
    }
   ],
   "source": [
    "# csv = '/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/B_blocks/test_scan_178/llp_simulation_results/incremental_results/llp_0130_result.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def merge_csv(folder_path, out_path):\n",
    "    all_df = pd.DataFrame()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "    all_df.to_csv(out_path+ '/merged.csv', index=False)\n",
    "    print(f\"Merged {len(os.listdir(folder_path))} files into {out_path}\")\n",
    "    return all_df\n",
    "\n",
    "def merge_seed_csvs(folder_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    合并指定文件夹中不同seed的B_511和B_521 csv文件\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录，如果为None则输出到原文件夹\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 用于存储不同前缀的文件数据\n",
    "    file_groups = {}\n",
    "    \n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 只处理csv文件\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        # 匹配文件名模式：llp_0130_result.csv 或 llp_0131_result.csv 等\n",
    "        # 规范化文件名以便后续的正则能匹配到，例如: 'llp_0130_result.csv' -> 'llp_0130.csv'\n",
    "        match = re.search(r'(llp_\\d+)', filename)\n",
    "        if match:\n",
    "            filename = f\"{match.group(1)}.csv\"\n",
    "        # match = re.match(r'llp_\\d+\\.csv', filename)\n",
    "        print(f\"Processing file: {filename}, Match: {match}\")\n",
    "        \n",
    "        if match:\n",
    "            prefix = match.group(0)  # 获取前缀 (llp_0130.csv, llp_0131.csv, etc.)\n",
    "            \n",
    "            try:\n",
    "                # 读取csv文件\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # 添加一个列来记录原始文件名（可选）\n",
    "                # df['source_file'] = filename\n",
    "                \n",
    "                # 将数据添加到对应的分组中\n",
    "                if prefix not in file_groups:\n",
    "                    file_groups[prefix] = []\n",
    "                file_groups[prefix].append(df)\n",
    "                \n",
    "                print(f\"已读取: {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "    \n",
    "    # 合并并保存每个分组的数据\n",
    "    for prefix, dfs in file_groups.items():\n",
    "        if dfs:\n",
    "            try:\n",
    "                # 合并所有DataFrame\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "                # 输出文件路径\n",
    "                output_path = os.path.join(output_dir, f\"{prefix}.csv\")\n",
    "                \n",
    "                # 保存到csv文件\n",
    "                combined_df.to_csv(output_path, index=False)\n",
    "                print(f\"\\n已成功合并并保存: {output_path}\")\n",
    "                print(f\"合并了 {len(dfs)} 个文件\")\n",
    "                print(f\"总行数: {len(combined_df)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"合并 {prefix} 文件时出错: {e}\")\n",
    "        else:\n",
    "            print(f\"没有找到 {prefix} 相关的文件\")\n",
    "    \n",
    "    return file_groups\n",
    "\n",
    "def merge_seed_csvs_with_options(folder_path, output_dir=None, add_source_column=True):\n",
    "    \"\"\"\n",
    "    更灵活的合并函数，带有更多选项\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录\n",
    "    add_source_column (bool): 是否添加源文件名列\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 使用字典存储不同模式的文件\n",
    "    patterns = {\n",
    "        'B_511': re.compile(r'B_511_seed\\d+\\.csv'),\n",
    "        'B_521': re.compile(r'B_521_seed\\d+\\.csv')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matched_dfs = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if pattern.match(filename):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    if add_source_column:\n",
    "                        df['source_file'] = filename\n",
    "                    \n",
    "                    matched_dfs.append(df)\n",
    "                    print(f\"已读取: {filename} -> {pattern_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "        \n",
    "        if matched_dfs:\n",
    "            # 合并数据\n",
    "            combined_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "            \n",
    "            # 保存文件\n",
    "            output_path = os.path.join(output_dir, f\"{pattern_name}.csv\")\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            results[pattern_name] = {\n",
    "                'file_count': len(matched_dfs),\n",
    "                'total_rows': len(combined_df),\n",
    "                'output_path': output_path\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✅ {pattern_name}: 合并了 {len(matched_dfs)} 个文件\")\n",
    "            print(f\"   总行数: {len(combined_df)}\")\n",
    "            print(f\"   保存到: {output_path}\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  未找到 {pattern_name} 相关的文件\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例用法1：基本用法\n",
    "    folder = \"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/14TeV_B\"\n",
    "    result = merge_csv(folder, out_path='/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP')\n",
    "    \n",
    "    # 示例用法2：指定输出目录\n",
    "    # result = merge_seed_csvs_with_options(folder, output_dir=\"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B2025-12-03_2HDM_B_test\", add_source_column=False)\n",
    "    \n",
    "    # 示例用法3：使用更灵活的函数（不添加源文件名列）\n",
    "    # result = merge_seed_csvs_with_options(\n",
    "    #     folder, \n",
    "    #     output_dir=\"/path/to/output\",\n",
    "    #     add_source_column=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0747a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: all_llp_detect_all_detectors_cross_section.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 2.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 3.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 4.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 5.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 6.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 7.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 8.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 9.csv\n",
      "Reading file: all_llp_detect_all_detectors_cross_section copy 10.csv\n",
      "Merged 12 files into /media/ubuntu/SRPPS/Results/ALL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>tau</th>\n",
       "      <th>CODEX-b_acceptance</th>\n",
       "      <th>MATHUSLA_acceptance</th>\n",
       "      <th>FASER_acceptance</th>\n",
       "      <th>FASER2_acceptance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.30903</td>\n",
       "      <td>1.962910e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.30903</td>\n",
       "      <td>2.069870e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.30903</td>\n",
       "      <td>1.051380e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2453.967077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.30903</td>\n",
       "      <td>1.005900e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30903</td>\n",
       "      <td>9.441790e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19771</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.370250e+14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19772</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.370250e+13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19773</th>\n",
       "      <td>3.09030</td>\n",
       "      <td>2.017120e+13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19774</th>\n",
       "      <td>3.09030</td>\n",
       "      <td>2.017120e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19775</th>\n",
       "      <td>3.09030</td>\n",
       "      <td>1.602260e+15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19776 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             m           tau  CODEX-b_acceptance  MATHUSLA_acceptance  \\\n",
       "0      0.30903  1.962910e-02                 0.0                  0.0   \n",
       "1      0.30903  2.069870e-03                 0.0                  0.0   \n",
       "2      0.30903  1.051380e+02                 0.0                  0.0   \n",
       "3      0.30903  1.005900e+01                 0.0                  0.0   \n",
       "4      0.30903  9.441790e-03                 0.0                  0.0   \n",
       "...        ...           ...                 ...                  ...   \n",
       "19771  1.00000  2.370250e+14                 0.0                  0.0   \n",
       "19772  1.00000  2.370250e+13                 0.0                  0.0   \n",
       "19773  3.09030  2.017120e+13                 0.0                  0.0   \n",
       "19774  3.09030  2.017120e+12                 0.0                  0.0   \n",
       "19775  3.09030  1.602260e+15                 0.0                  0.0   \n",
       "\n",
       "       FASER_acceptance  FASER2_acceptance  \n",
       "0                   0.0           0.000000  \n",
       "1                   0.0           0.000000  \n",
       "2                   0.0        2453.967077  \n",
       "3                   0.0           0.000000  \n",
       "4                   0.0           0.000000  \n",
       "...                 ...                ...  \n",
       "19771               0.0           0.000000  \n",
       "19772               0.0           0.000000  \n",
       "19773               0.0           0.000000  \n",
       "19774               0.0           0.000000  \n",
       "19775               0.0           0.000000  \n",
       "\n",
       "[19776 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_B(folder, out):\n",
    "    all_df = pd.DataFrame()\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Reading file: {filename}\")\n",
    "            all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "    all_df.to_csv(out+ '/merged.csv', index=False)\n",
    "    print(f\"Merged {len(os.listdir(folder))} files into {out}\")\n",
    "    return all_df\n",
    "\n",
    "merge_B('/media/ubuntu/SRPPS/Results/ALL/', '/media/ubuntu/SRPPS/Results/ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b41351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=3633033, step=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B/13TeV/2025-12-27_B_13TeV/merged_B.csv')\n",
    "print(df[:-1].index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
