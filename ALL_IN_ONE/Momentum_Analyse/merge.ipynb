{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544499a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_511_seed000.csv\n",
      "/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/14TeV_B/B_511_seed000.csv\n",
      "已读取: B_511_seed000.csv -> B_511\n",
      "B_521_seed000.csv\n",
      "\n",
      "✅ B_511: 合并了 1 个文件\n",
      "   总行数: 1730626\n",
      "   保存到: /media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/B_511.csv\n",
      "B_511_seed000.csv\n",
      "B_521_seed000.csv\n",
      "/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/14TeV_B/B_521_seed000.csv\n",
      "已读取: B_521_seed000.csv -> B_521\n",
      "\n",
      "✅ B_521: 合并了 1 个文件\n",
      "   总行数: 1733661\n",
      "   保存到: /media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/B_521.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def merge_seed_csvs(folder_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    合并指定文件夹中不同seed的B_511和B_521 csv文件\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录，如果为None则输出到原文件夹\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 用于存储不同前缀的文件数据\n",
    "    file_groups = {}\n",
    "    \n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 只处理csv文件\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "            \n",
    "        # 匹配文件名模式：B_511_seedXXXX.csv 或 B_521_seedXXXX.csv\n",
    "        match = re.match(r'(B_5[12]1)_seed\\d+\\.csv', filename)\n",
    "        \n",
    "        if match:\n",
    "            prefix = match.group(1)  # 获取前缀 (B_511 或 B_521)\n",
    "            \n",
    "            try:\n",
    "                # 读取csv文件\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # 添加一个列来记录原始文件名（可选）\n",
    "                df['source_file'] = filename\n",
    "                \n",
    "                # 将数据添加到对应的分组中\n",
    "                if prefix not in file_groups:\n",
    "                    file_groups[prefix] = []\n",
    "                file_groups[prefix].append(df)\n",
    "                \n",
    "                print(f\"已读取: {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "    \n",
    "    # 合并并保存每个分组的数据\n",
    "    for prefix, dfs in file_groups.items():\n",
    "        if dfs:\n",
    "            try:\n",
    "                # 合并所有DataFrame\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "                # 输出文件路径\n",
    "                output_path = os.path.join(output_dir, f\"{prefix}.csv\")\n",
    "                \n",
    "                # 保存到csv文件\n",
    "                combined_df.to_csv(output_path, index=False)\n",
    "                print(f\"\\n已成功合并并保存: {output_path}\")\n",
    "                print(f\"合并了 {len(dfs)} 个文件\")\n",
    "                print(f\"总行数: {len(combined_df)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"合并 {prefix} 文件时出错: {e}\")\n",
    "        else:\n",
    "            print(f\"没有找到 {prefix} 相关的文件\")\n",
    "    \n",
    "    return file_groups\n",
    "\n",
    "def merge_seed_csvs_with_options(folder_path, output_dir=None, add_source_column=True):\n",
    "    \"\"\"\n",
    "    更灵活的合并函数，带有更多选项\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录\n",
    "    add_source_column (bool): 是否添加源文件名列\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 使用字典存储不同模式的文件\n",
    "    patterns = {\n",
    "        'B_511': re.compile(r'B_511_seed\\d+\\.csv'),\n",
    "        'B_521': re.compile(r'B_521_seed\\d+\\.csv')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matched_dfs = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            print(filename)\n",
    "            if pattern.match(filename):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(file_path)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    if add_source_column:\n",
    "                        df['source_file'] = filename\n",
    "                    \n",
    "                    matched_dfs.append(df)\n",
    "                    print(f\"已读取: {filename} -> {pattern_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "        \n",
    "        if matched_dfs:\n",
    "            # 合并数据\n",
    "            combined_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "            \n",
    "            # 保存文件\n",
    "            output_path = os.path.join(output_dir, f\"{pattern_name}.csv\")\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            results[pattern_name] = {\n",
    "                'file_count': len(matched_dfs),\n",
    "                'total_rows': len(combined_df),\n",
    "                'output_path': output_path\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✅ {pattern_name}: 合并了 {len(matched_dfs)} 个文件\")\n",
    "            print(f\"   总行数: {len(combined_df)}\")\n",
    "            print(f\"   保存到: {output_path}\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  未找到 {pattern_name} 相关的文件\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例用法1：基本用法\n",
    "    folder = \"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/14TeV_B/\"\n",
    "    # result = merge_seed_csvs(folder)\n",
    "    \n",
    "    # 示例用法2：指定输出目录\n",
    "    result = merge_seed_csvs_with_options(folder, output_dir=\"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP\", add_source_column=False)\n",
    "    \n",
    "    # 示例用法3：使用更灵活的函数（不添加源文件名列）\n",
    "    # result = merge_seed_csvs_with_options(\n",
    "    #     folder, \n",
    "    #     output_dir=\"/path/to/output\",\n",
    "    #     add_source_column=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccdb61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 files into /media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP\n"
     ]
    }
   ],
   "source": [
    "# csv = '/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/B_blocks/test_scan_178/llp_simulation_results/incremental_results/llp_0130_result.csv'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def merge_csv(folder_path, out_path):\n",
    "    all_df = pd.DataFrame()\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "    all_df.to_csv(out_path+ '/merged.csv', index=False)\n",
    "    print(f\"Merged {len(os.listdir(folder_path))} files into {out_path}\")\n",
    "    return all_df\n",
    "\n",
    "def merge_seed_csvs(folder_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    合并指定文件夹中不同seed的B_511和B_521 csv文件\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录，如果为None则输出到原文件夹\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 用于存储不同前缀的文件数据\n",
    "    file_groups = {}\n",
    "    \n",
    "    # 遍历文件夹中的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 只处理csv文件\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        # 匹配文件名模式：llp_0130_result.csv 或 llp_0131_result.csv 等\n",
    "        # 规范化文件名以便后续的正则能匹配到，例如: 'llp_0130_result.csv' -> 'llp_0130.csv'\n",
    "        match = re.search(r'(llp_\\d+)', filename)\n",
    "        if match:\n",
    "            filename = f\"{match.group(1)}.csv\"\n",
    "        # match = re.match(r'llp_\\d+\\.csv', filename)\n",
    "        print(f\"Processing file: {filename}, Match: {match}\")\n",
    "        \n",
    "        if match:\n",
    "            prefix = match.group(0)  # 获取前缀 (llp_0130.csv, llp_0131.csv, etc.)\n",
    "            \n",
    "            try:\n",
    "                # 读取csv文件\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # 添加一个列来记录原始文件名（可选）\n",
    "                # df['source_file'] = filename\n",
    "                \n",
    "                # 将数据添加到对应的分组中\n",
    "                if prefix not in file_groups:\n",
    "                    file_groups[prefix] = []\n",
    "                file_groups[prefix].append(df)\n",
    "                \n",
    "                print(f\"已读取: {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "    \n",
    "    # 合并并保存每个分组的数据\n",
    "    for prefix, dfs in file_groups.items():\n",
    "        if dfs:\n",
    "            try:\n",
    "                # 合并所有DataFrame\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                \n",
    "                # 输出文件路径\n",
    "                output_path = os.path.join(output_dir, f\"{prefix}.csv\")\n",
    "                \n",
    "                # 保存到csv文件\n",
    "                combined_df.to_csv(output_path, index=False)\n",
    "                print(f\"\\n已成功合并并保存: {output_path}\")\n",
    "                print(f\"合并了 {len(dfs)} 个文件\")\n",
    "                print(f\"总行数: {len(combined_df)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"合并 {prefix} 文件时出错: {e}\")\n",
    "        else:\n",
    "            print(f\"没有找到 {prefix} 相关的文件\")\n",
    "    \n",
    "    return file_groups\n",
    "\n",
    "def merge_seed_csvs_with_options(folder_path, output_dir=None, add_source_column=True):\n",
    "    \"\"\"\n",
    "    更灵活的合并函数，带有更多选项\n",
    "    \n",
    "    参数:\n",
    "    folder_path (str): 包含csv文件的文件夹路径\n",
    "    output_dir (str): 输出文件的目录\n",
    "    add_source_column (bool): 是否添加源文件名列\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = folder_path\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 使用字典存储不同模式的文件\n",
    "    patterns = {\n",
    "        'B_511': re.compile(r'B_511_seed\\d+\\.csv'),\n",
    "        'B_521': re.compile(r'B_521_seed\\d+\\.csv')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matched_dfs = []\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if pattern.match(filename):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    if add_source_column:\n",
    "                        df['source_file'] = filename\n",
    "                    \n",
    "                    matched_dfs.append(df)\n",
    "                    print(f\"已读取: {filename} -> {pattern_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"读取文件 {filename} 时出错: {e}\")\n",
    "        \n",
    "        if matched_dfs:\n",
    "            # 合并数据\n",
    "            combined_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "            \n",
    "            # 保存文件\n",
    "            output_path = os.path.join(output_dir, f\"{pattern_name}.csv\")\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "            \n",
    "            results[pattern_name] = {\n",
    "                'file_count': len(matched_dfs),\n",
    "                'total_rows': len(combined_df),\n",
    "                'output_path': output_path\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✅ {pattern_name}: 合并了 {len(matched_dfs)} 个文件\")\n",
    "            print(f\"   总行数: {len(combined_df)}\")\n",
    "            print(f\"   保存到: {output_path}\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  未找到 {pattern_name} 相关的文件\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例用法1：基本用法\n",
    "    folder = \"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP/14TeV_B\"\n",
    "    result = merge_csv(folder, out_path='/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Test/14TeV_LLP')\n",
    "    \n",
    "    # 示例用法2：指定输出目录\n",
    "    # result = merge_seed_csvs_with_options(folder, output_dir=\"/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B2025-12-03_2HDM_B_test\", add_source_column=False)\n",
    "    \n",
    "    # 示例用法3：使用更灵活的函数（不添加源文件名列）\n",
    "    # result = merge_seed_csvs_with_options(\n",
    "    #     folder, \n",
    "    #     output_dir=\"/path/to/output\",\n",
    "    #     add_source_column=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0747a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: B_511.csv\n",
      "Reading file: B_521.csv\n",
      "Merged 4 files into /media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B/13TeV/2025-12-27_B_13TeV\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau_input</th>\n",
       "      <th>decay_z</th>\n",
       "      <th>decay_x</th>\n",
       "      <th>e</th>\n",
       "      <th>pz</th>\n",
       "      <th>seed</th>\n",
       "      <th>decay_y</th>\n",
       "      <th>py</th>\n",
       "      <th>decay_t</th>\n",
       "      <th>px</th>\n",
       "      <th>total_events</th>\n",
       "      <th>m</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>-0.015358</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>44.1493</td>\n",
       "      <td>-43.82970</td>\n",
       "      <td>659230</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.339456</td>\n",
       "      <td>0.015470</td>\n",
       "      <td>-0.355425</td>\n",
       "      <td>2</td>\n",
       "      <td>5.27958</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>1.913640</td>\n",
       "      <td>0.210519</td>\n",
       "      <td>96.8466</td>\n",
       "      <td>95.64330</td>\n",
       "      <td>659230</td>\n",
       "      <td>-0.193009</td>\n",
       "      <td>-9.646530</td>\n",
       "      <td>1.937720</td>\n",
       "      <td>10.521700</td>\n",
       "      <td>4</td>\n",
       "      <td>5.27958</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>0.035220</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>123.8110</td>\n",
       "      <td>123.67000</td>\n",
       "      <td>659230</td>\n",
       "      <td>-0.000753</td>\n",
       "      <td>-2.644090</td>\n",
       "      <td>0.035260</td>\n",
       "      <td>0.027057</td>\n",
       "      <td>7</td>\n",
       "      <td>5.27958</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>-0.123037</td>\n",
       "      <td>-0.008053</td>\n",
       "      <td>25.6489</td>\n",
       "      <td>-25.02540</td>\n",
       "      <td>659230</td>\n",
       "      <td>0.005010</td>\n",
       "      <td>1.019020</td>\n",
       "      <td>0.126102</td>\n",
       "      <td>-1.637920</td>\n",
       "      <td>10</td>\n",
       "      <td>5.27958</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>0.108063</td>\n",
       "      <td>0.151274</td>\n",
       "      <td>12.7241</td>\n",
       "      <td>6.64133</td>\n",
       "      <td>659230</td>\n",
       "      <td>0.030382</td>\n",
       "      <td>1.867220</td>\n",
       "      <td>0.207037</td>\n",
       "      <td>9.297010</td>\n",
       "      <td>11</td>\n",
       "      <td>5.27958</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633029</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>-79.005200</td>\n",
       "      <td>-0.695648</td>\n",
       "      <td>387.7580</td>\n",
       "      <td>-387.64200</td>\n",
       "      <td>801070</td>\n",
       "      <td>1.441670</td>\n",
       "      <td>7.073630</td>\n",
       "      <td>79.028700</td>\n",
       "      <td>-3.413230</td>\n",
       "      <td>39983</td>\n",
       "      <td>5.27925</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633030</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>-0.212545</td>\n",
       "      <td>0.064415</td>\n",
       "      <td>14.8988</td>\n",
       "      <td>-13.29090</td>\n",
       "      <td>801070</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>1.109640</td>\n",
       "      <td>0.238258</td>\n",
       "      <td>4.028000</td>\n",
       "      <td>39991</td>\n",
       "      <td>5.27925</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633031</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>-0.184012</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>45.1873</td>\n",
       "      <td>-44.56610</td>\n",
       "      <td>801070</td>\n",
       "      <td>-0.021732</td>\n",
       "      <td>-5.263270</td>\n",
       "      <td>0.186577</td>\n",
       "      <td>0.423139</td>\n",
       "      <td>39992</td>\n",
       "      <td>5.27925</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633032</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>-5.533700</td>\n",
       "      <td>0.109072</td>\n",
       "      <td>44.1473</td>\n",
       "      <td>-43.82200</td>\n",
       "      <td>801070</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>0.055866</td>\n",
       "      <td>5.574790</td>\n",
       "      <td>0.863750</td>\n",
       "      <td>39998</td>\n",
       "      <td>5.27925</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633033</th>\n",
       "      <td>3567550.0</td>\n",
       "      <td>22.203000</td>\n",
       "      <td>0.995668</td>\n",
       "      <td>126.4620</td>\n",
       "      <td>126.19600</td>\n",
       "      <td>801070</td>\n",
       "      <td>0.472872</td>\n",
       "      <td>2.687680</td>\n",
       "      <td>22.249700</td>\n",
       "      <td>5.659110</td>\n",
       "      <td>39999</td>\n",
       "      <td>5.27925</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3633034 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tau_input    decay_z   decay_x         e         pz    seed  \\\n",
       "0        3567550.0  -0.015358 -0.000125   44.1493  -43.82970  659230   \n",
       "1        3567550.0   1.913640  0.210519   96.8466   95.64330  659230   \n",
       "2        3567550.0   0.035220  0.000008  123.8110  123.67000  659230   \n",
       "3        3567550.0  -0.123037 -0.008053   25.6489  -25.02540  659230   \n",
       "4        3567550.0   0.108063  0.151274   12.7241    6.64133  659230   \n",
       "...            ...        ...       ...       ...        ...     ...   \n",
       "3633029  3567550.0 -79.005200 -0.695648  387.7580 -387.64200  801070   \n",
       "3633030  3567550.0  -0.212545  0.064415   14.8988  -13.29090  801070   \n",
       "3633031  3567550.0  -0.184012  0.001747   45.1873  -44.56610  801070   \n",
       "3633032  3567550.0  -5.533700  0.109072   44.1473  -43.82200  801070   \n",
       "3633033  3567550.0  22.203000  0.995668  126.4620  126.19600  801070   \n",
       "\n",
       "          decay_y        py    decay_t         px  total_events        m   id  \n",
       "0        0.000119  0.339456   0.015470  -0.355425             2  5.27958  511  \n",
       "1       -0.193009 -9.646530   1.937720  10.521700             4  5.27958  511  \n",
       "2       -0.000753 -2.644090   0.035260   0.027057             7  5.27958  511  \n",
       "3        0.005010  1.019020   0.126102  -1.637920            10  5.27958  511  \n",
       "4        0.030382  1.867220   0.207037   9.297010            11  5.27958  511  \n",
       "...           ...       ...        ...        ...           ...      ...  ...  \n",
       "3633029  1.441670  7.073630  79.028700  -3.413230         39983  5.27925  521  \n",
       "3633030  0.017745  1.109640   0.238258   4.028000         39991  5.27925  521  \n",
       "3633031 -0.021732 -5.263270   0.186577   0.423139         39992  5.27925  521  \n",
       "3633032  0.007055  0.055866   5.574790   0.863750         39998  5.27925  521  \n",
       "3633033  0.472872  2.687680  22.249700   5.659110         39999  5.27925  521  \n",
       "\n",
       "[3633034 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_B(folder, out):\n",
    "    all_df = pd.DataFrame()\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Reading file: {filename}\")\n",
    "            all_df = pd.concat([all_df, df], ignore_index=True)\n",
    "    all_df.to_csv(out+ '/merged_B.csv', index=False)\n",
    "    print(f\"Merged {len(os.listdir(folder))} files into {out}\")\n",
    "    return all_df\n",
    "\n",
    "merge_B('/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B/13TeV/2025-12-27_B_13TeV/', '/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B/13TeV/2025-12-27_B_13TeV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b41351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=3633033, step=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/media/ubuntu/6156e08b-fdb1-4cde-964e-431f74a6078e/Files/LLP_DATA/Decay_B/13TeV/2025-12-27_B_13TeV/merged_B.csv')\n",
    "print(df[:-1].index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
